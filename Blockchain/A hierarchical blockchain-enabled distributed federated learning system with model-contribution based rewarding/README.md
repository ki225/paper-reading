# A hierarchical blockchain-enabled distributed federated learning system with model-contribution based rewarding

- https://www.sciencedirect.com/science/article/pii/S2352864824000853


## 簡介

提出了一種包含公共區塊鏈層和聯盟區塊鏈層的 HBDFL 系統，為 DFL 問題提供了一個通用的框架，並設計了 MCRM、PoETA、DRTM 和 ADTM 機制，分別解決了 DFL 中的激勵、安全、效率和準確性問題。

## 系統設計
### 3.1 系統模型

如圖 1 所示，HBDFL 系統採用聯盟-公共區塊鏈結構。該系統包含兩種不同的區塊鏈網路：公共區塊鏈網路和聯盟區塊鏈網路，並具有三種節點類型：**發起者 (Sponsor)**、**驗證者 (Validator)** 和 **提交者 (Committer)**。

![image](https://hackmd.io/_uploads/ByVLy6LQJg.png)

#### 3.1.1 發起者 (Sponsor)
發起者是希望訓練特定模型的實體或個人。公共區塊鏈中的每個節點都可以作為發起者，並將其 DFL 任務發布到區塊鏈平台。發起者透過提供一個待訓練的模型以及選擇驗證者的標準來初始化 DFL 的工作流程。驗證者透過 **經過時間與準確性證明 (PoETA)** 共識機制來選擇新區塊。在選擇區塊後，將觸發發起者的智能合約，並透過公共區塊鏈為參與訓練的提交者提供基於加密貨幣的獎勵。

#### 3.1.2 驗證者 (Validator)
驗證者負責交易驗證、模型準確性確認以及將本地模型聚合成全域模型。每個驗證者的註冊和取消需要超過 50% 發起者的批准。驗證者會在每個時間週期 \( T_p \) 生成一個區塊。

#### 3.1.3 提交者 (Committer)
提交者擁有本地數據集和 AI 模型訓練所需的計算能力。為了保護訓練數據的隱私，在提交者執行模型訓練時會向模型參數添加  $\epsilon$ -差分隱私 (DP) 噪聲。提交者執行以下三個步驟：

1. 從聯盟區塊鏈網路下載最新的全域模型。
2. 使用其私有數據集開始訓練模型。為了保護訓練數據隱私，訓練過程中將採用 $\epsilon$ -差分隱私噪聲。
3. 將訓練好的本地模型及其他相關資訊（如模型準確性、提交者的數位簽名等）打包為一筆交易。需要注意的是，由於鏈上存儲的限制，訓練好的模型應存儲於鏈下的去中心化存儲網路中。這樣做的話，僅需在聯盟鏈上存儲模型的索引地址。

### 聯盟鏈交易區塊
- 區塊
    - 區塊頭: 記錄了區塊的元數據，包括時間戳、前一區塊的哈希值、全域模型的地址等
    - 區塊體: 由提交者生成的一組交易。每筆交易包括交易 ID、請求者的公共金鑰、數位簽名等。
        - 除了與聯邦學習（FL）模型相關的交易外，還有其他類型的交易，例如向發起者請求獎勳和驗證者的註冊與取消。
- 只有驗證者是執行聯盟區塊鏈共識演算法來生成和驗證區塊的節點。聯盟區塊鏈的設計目的是驗證、聚合並存儲 FL 模型【2】。此外，聯盟區塊鏈採用了提議的 PoETA 和 DRTM 演算法來確保系統的安全性、模型的準確性和卓越的系統吞吐量。公共區塊鏈則設計為有效激勵公眾加入並貢獻於 DFL 任務。

![image](https://hackmd.io/_uploads/SkMblpUQ1e.png)

### 系統工作流程

1. 發起者通過向聯盟區塊鏈提交智能合約來初始化過程。智能合約包含了關鍵資訊，如模型架構和數據集類型。
2. 基於發起者的請求，驗證者生成一個創世區塊，其中包含待訓練的初始化模型和數據集的要求。初始模型架構和生成創世區塊的驗證者是由所有發起者提前離線選擇的。
3. 擁有本地訓練數據集和計算資源的提交者，下載來自聯盟區塊鏈的最新區塊（即最新的全域模型），並執行本地模型訓練【2】。鑑於區塊鏈的儲存容量有限，只有 FL 模型的地址會存儲在鏈上，模型參數可以存儲在去中心化存儲網路中，例如 IPFS【21】。
4. 訓練完成後，模型的地址將被打包為一筆交易並發送給隨機選中的驗證者。需要注意的是，每個驗證者都有一個交易上限，因此當交易數量達到上限時，該驗證者將無法被選中。
5. 每個驗證者在等待預定的等待時間 \(T_p\) 後，收集並將接收到的交易打包成新區塊。新區塊將通過 DRTM 進行驗證，並透過 PoETA 共識演算法選擇添加到聯盟區塊鏈中。接下來，生成該區塊的驗證者會定期對新區塊中的本地模型進行模型平均（FedAvg）操作，以生成全域模型【23】。
6. 最後，提交者將透過設計的基於加密貨幣的激勳機制（MCRM）在公共區塊鏈上獲得發起者的獎勳。

![image](https://hackmd.io/_uploads/HJTXZ6UX1e.png)


### 3.2. PoETA 共識機制  
- 共識機制目的: 
    - 讓驗證者達成對區塊的共識
    - 所有對該區塊作出貢獻的提交者將由發起者獲得獎勳
- 起源: 最初由英特爾在工業領域引入並使用的 Proof of Elapsed Time (PoET)【24】共識機制，針對區塊鏈網路，旨在減少傳統共識機制所涉及的能源和計算資源消耗。
- 說明
    - 每個驗證者在將新區塊附加到區塊鏈之前會生成一個隨機等待時間，這個等待時間需要遵循一個由系統預先確定的概率分佈 \( F \)。在每個共識周期中，等待時間最短的驗證者會被選中。
- 修改
    - 作者引入「模型準確度」
        - 使一個等待時間短且模型準確度高的驗證者將被選中，將其區塊附加到聯盟區塊鏈上。
        - 具體而言為每個 DFL 訓練共識周期中的新區塊定義了一個區塊分數 \( s_i \)。這個周期是每個新區塊被添加到聯盟區塊鏈之間的時間間隔。區塊分數計算公式為：
$$
s_i = T_i \cdot (1 - a_i), \quad i \in N, \tag{1}
$$
    - \( T_i \) 是驗證者 \( i \) 的隨機等待時間，\( a_i \) 是該驗證者處的全域模型的準確度。
    - 將分數最低的新區塊附加到聯盟區塊鏈。

> 可能會發生網路錯誤或延遲，導致分叉鏈問題，即多條子鏈跟隨相同的父區塊。在這種情況下，將比較每條子鏈上所有區塊分數的總和，並選擇總分最低的子鏈作為主鏈。

### 3.3. 基於分散式聲譽的吞吐量機制 (DRTM) 
#### 背景
- 驗證時機
    - 將提交者發送的每個交易添加到區塊之前
    - 將其他驗證者發送的每個區塊附加到主鏈之前
- 問題: 當模型包含大量參數時，驗證模型所需的時間和計算成本是相當大的 -> 作者設計了 DRTM 來確定哪些交易或區塊需要進行檢查，以減少驗證成本，同時提高系統的吞吐量。
#### 介紹
- 角色
    - 請求者
        - 對其提交進行驗證的提交者 or
        - 需要對其生成的區塊進行驗證的驗證者。
- 紀錄
    - 每個驗證者都維護一個表格
    - 紀錄內容
        - 請求者的公鑰和聲譽分數。
            - 我們定義了第 \( j \) 個請求者節點的驗證分數 \( V_j \)
            - 第 \( j \) 個請求者節點的聲譽分數 \( r_j \) 
            - 聲譽分數的上限 \( c \)。
                - 聲譽分數分為從 0 到 \( c \) 的多個級別，其中較高的聲譽分數對應於較低的驗證概率。

#### DRTM 演算法
1. 初始化聲譽分數為零。當一個新交易到達驗證者時，驗證者通過請求者的公鑰從鍵值映射中獲取請求者的聲譽分數。 
2. 請求者的驗證分數在 0 到 \( c \) 之間隨機均勻地取值。  
3. 如果驗證分數大於聲譽分數，則對該交易或區塊進行驗證。如果該交易或區塊有效，則請求者的聲譽分數增加 1。如果發現無效的交易或區塊，則相應的聲譽分數重置為 0。  
    - 從原則上講，請求者的聲譽分數越高，驗證的概率越低。


![image](https://hackmd.io/_uploads/B1wcmT8m1l.png)

```
procedure DRTM(rj, c, Vj)
    Initialize rj = 0, c
    while new transactions or blocks arrive at the validator do
        Vj = Random(0, c)
        if rj < Vj then
            Validate the transaction or block
            if the transaction or block is valid then
                if rj < c then
                    rj = rj + 1
                else
                    rj = c
                end if
            else
                rj = 0
            end if
        end if
        Update blockchain
    end while
end procedure
```

### 3.4. 依賴準確度的吞吐量機制 (ADTM)
- 執行角色
    - 驗證者
- 目的: 旨在改善吞吐量的同時確保系統的準確度。
- 功能: 監控模型準確度，同時保持吞吐量在預定範圍內 (νmin, νmax)



儘管不同的驗證者有不同的封包時間 Tp 限制，但其目標是平衡全球模型的準確度與吞吐量性能之間的權衡。

吞吐量 ν 被定義為每秒儲存在主鏈中的交易數量，如下所示：

$$
ν_i = \frac{\sum_{l=1}^{L} M_i^l}{\sum_{l=1}^{L} (T_i^{l_{packet}} + T_i^{l_{wait}} + T_j^{l_{communicate}})}
$$

其中，\(M_i^l\) 是第 \(l\) 個區塊中的交易數量，\(T_i^{l_{packet}}\) 代表封包和驗證第 \(l\) 個區塊的時間，\(T_i^{l_{wait}}\) 是執行 PoETA 共識機制時的等待時間，\(T_j^{l_{communicate}}\) 只有當區塊由其他驗證者提議時才存在。由於通信延遲和異步共識機制，ν 的值在不同的驗證者之間可能會有所不同。

公式 (2) 表示有兩種方式可以調整 ν。一方面，驗證者可以通過改變區塊大小 M 來調整 ν；另一方面，驗證者可以改變 Tpacket，這表示等待接收交易的時間。前者會影響驗證者之間的通信延遲，這可能會導致分叉鏈的數量增加，甚至可能將某些驗證者隔離在網路之外。相反，不管驗證者如何修改 Tpacket，其他驗證者的吞吐量不會受到影響。

#### 演算法
![image](https://hackmd.io/_uploads/BkB-IaU71x.png)

- 變數
    - acurr、amax、k、klimit 和 Tp 分別代表當前區塊的準確度、先前區塊的最大準確度、準確度的變化量、系統定義的準確度範圍，以及封包時間限制。
- 步驟
    1. ADTM 開始時，根據準確度的趨勢計算準確度變化量 k。
    2. 只有當 |k| ≥ klimit 時，驗證者才會調整封包時間 Tp。
    3. 如果吞吐量保持在期望的範圍內，則 Tp 的調整會基於準確度的趨勢及距離預定吞吐量限制的距離來進行。否則，Tp 的調整僅考慮吞吐量性能。

### 3.5. 基於模型貢獻的獎勳機制 (MCRM)

- 背景
    - 各個提交者的私有數據大小和計算能力可能有很大差異，那些擁有大量訓練數據集和高計算能力的提交者可能不願意參與聯邦學習 (FL) 任務。
- 執行地點: 在智能合約中執行，並通過公共區塊鏈層由贊助者根據 MCRM 分配加密貨幣獎勳給提交者（**演算法 3**）。
- 獎勵說明
    - 需要注意的是，當模型的準確度已經很高，或者已經運行了大量訓練時，對於提交者來說，很難進一步提高模型的準確度。
    - 為了公平考慮，對於能夠改善相對較好訓練模型的提交者，會分配更高的獎勳。

#### 演算法
- 參數
    -  \(l\)、\(a_{pre}\)、\(\lambda\)、\(s\) 和 \(\psi\) 分別表示聯盟鏈中先前區塊的數量、前一區塊中全局模型的準確度、擴展係數、補償係數和將獎勳分數轉換為加密貨幣單位的獎勳係數。
- 步驟
    1. 當新區塊添加到公共鏈上後，該區塊在公共鏈上的背書(endorsement)次數會增加 1。
        - 背書次數是指區塊成功被所有驗證者驗證的次數。
    2. 系統使用演算法第 7 行中的公式計算分數 \(score_i\)。在公式的右側，第一項衡量的是新交易 \(i\) （本地模型）相對於 \(a_{pre}\) （先前全局模型的準確度）的模型準確度改善程度，而第二項衡量的是聯盟鏈的長度，反映了 FL 模型訓練的 epoch 次數。


```py=
def MCRM(apre, λ, s, l, ψ):
    Initialize λ, s, ψ  # 初始化擴展係數 λ、補償係數 s 和獎勳轉換係數 ψ
    
    while True:  # 持續檢查是否有新交易或區塊到達
        new_block_added = check_new_block_added()  # 檢查是否有新區塊加入

        if new_block_added:
            ai = get_accuracy_of_new_block()  # 獲取新區塊的準確度 ai
            λi = λ / (- log(apre)) # 計算新的 λi 值，這是根據之前全局模型準確度 apre 計算的
            sl = s * log(l) # 計算基於聯盟鏈長度 l 的補償係數 sl
            scorei = λi * (ai - apre) + sl # 計算模型改善分數 scorei，根據準確度的提升和鏈長
            ψi = ψ * scorei # 計算最終的獎勳值 ψi，根據 scorei 和轉換係數 ψ
            
            send_reward_to_committer(ψi)  # 贊助者將計算出的獎勳 ψi 發送給該提交者
```

![image](https://hackmd.io/_uploads/HyRkq6U7Jl.png)

#### 利益計算公式


1. 計算擴展係數：
    - 若上一個全局模型的準確度越高，則 lambda_i 越高 -> 上一個全局模型非常準確，那麼當前模型的貢獻對最終獎勳的影響會被放大。
    $$
   \lambda_i = \lambda / (- \log(a_{\text{pre}}))
   $$
2. 計算補償係數：
   $$
   s_l = s \cdot \log(l)
   $$
3. 計算提交者的貢獻分數：
   $$
   \text{score}_i = \lambda_i \cdot (a_i - a_{\text{pre}}) + s_l
   $$
4. 計算最終的獎勳：
   $$
   \psi_i = \psi \cdot \text{score}_i
   $$

### 註解：
- $\lambda$ 是一個常數，用來調整準確度改進對最終獎勳的影響。
- \( s \) 是補償係數，影響聯盟鏈長度對獎勳的貢獻。
- $\psi$ 是將計算出的分數轉換為具體獎勳（如加密貨幣）的轉換係數。
- $a_i - a_{\text{pre}}$ 表示模型準確度的改進量，這會影響提交者的獎勳。
- \( l \) 是聯盟鏈的長度，反映了訓練的進度。


