# 文章閱讀 FLoBC: A Decentralized Blockchain-Based Federated Learning Framework

- 原文: https://arxiv.org/abs/2112.11873
- 時間: 2021
- Github: https://github.com/Oschart/FLoBC



## BACKGROUND AND RELATED WORK

1. **區塊鏈和聯邦學習的結合潛力**：
   - 區塊鏈可實現去中心化共識，而聯邦學習結合區塊鏈可進一步增強去中心化和隱私性。
   - 去中心化聯邦學習的目標是避免使用中央伺服器進行數據收集與模型訓練，提高隱私並分散計算負載。
2. **現有方法的通信成本挑戰**：
   - 常用的All-Reduce方案會導致通信成本高達O(n²)，尤其是在大量節點的情況下。
   - 利用網路拓撲（如環狀、樹狀、圖形拓撲）可以降低通信成本，但更新需經過多跳傳遞，影響收斂速度。
3. **八卦協議（Gossip Protocol）與驗證節點引入**：
   - 八卦協議透過隨機節點間更新傳遞減少通信負擔，但增加了訓練節點的驗證負擔。
   - 作者提出的框架透過引入驗證節點（類似加密貨幣中的礦工）來分擔驗證責任，並建立更易管理的信任系統。
4. **區塊鏈在聯邦學習中的應用**：
   - 使用區塊鏈作為通信基礎設施，方便更新的上傳與追蹤，並獎勵用戶參與模型訓練或驗證，使更新不可篡改。
   - 例子包括智能合約、工作量證明（PoW）、以及針對車載訓練的點對點檔案共享系統。
5. **作者提出的框架的特點**：
   - 使用權益證明（PoS）作為輕量化的替代方案，減少計算負擔。
   - 提供通用性，不受特定任務或用途限制，只要滿足必要的共識假設即可。

##  METHODOLOGY
為實現去中心化聯邦學習系統，作者制定了一些主要標準作為系統功能和解決方案的基礎，包括以下幾點：

- **通用性**：系統設計應對模型的具體細節保持中立，只需確保模型與梯度下降兼容。
- **去中心化**：系統的運行需透過去中心化共識進行。
- **不共享數據**：各節點僅共享見解，而不需分享其原始數據。


以下將討論分散式學習系統的三個主要維度：**並行性**、**中心化程度**以及**同步性**。

###  Parallelism
### 並行性
> 在機器學習過程中，訓練模型往往是最耗資源的部分，因此分散式學習如何幫助分解這個過程呢？

分散式學習一般有兩種並行模式來加速此過程：**模型並行**和**數據並行**
> 在作者的系統中，我們選擇了數據並行的方式。

- **模型並行**：
    - 模型結構本身被拆分或分配到多個節點上，這一模式通常稱為「分割學習 (Split Learning)」。
        - 例如
            - 在神經網路的情況下，可以在特定層（稱為「切層」）進行拆分，每個訓練者被分配到一組連續的層，然後根據模型架構的層順序將所有層組合併起來。
        - 優勢
            - 數據隱私保護
            - 允許計算能力較低的節點參與訓練。
        - 限制
            - 存在一定的順序限制，最終會限制其可擴展性。
            > 對於作者的系統而言，模型並行的主要缺點是它依賴於具體的模型和算法，使其難以用於通用的訓練系統。
- **數據並行**
    - 每個節點都會使用完整的模型進行訓練，但僅使用訓練數據的一部分。每個節點平行地進行訓練，然後將各個節點生成的模型更新彙總為一個全局模型。
    - 這種方法在隨機過程（如隨機梯度下降, SGD）上具有較好的並行處理能力，而隨機梯度下降被廣泛應用於分散式學習。此外，數據並行容易應用於任何適合並行計算的過程，例如梯度計算和近似計算。
    - 因此，作者選擇了這種並行模型，因為它大大簡化了通用模型結構，只需一個權重平坦的陣列即可重建完整模型。

### Degree of Centralization
在集中化的範圍上，有兩個極端：完全集中化和完全去中心化。
- 完全集中化的系統即是一個單一設備完成所有訓練過程。
- 經典的聯邦學習則採用一個中央伺服器或伺服器集群來管理其他節點的訓練。
- 本文的系統實現了一個更高去中心化程度的設計，類似於大多數的加密貨幣網路，包含了兩類節點：訓練者（trainer）和驗證者（validator）
    - 驗證者的角色類似於加密礦工，驗證者的主要任務是透過基於投票的去中心化共識維護基於區塊鏈網路的模型。

理想情況下，驗證者應該根據訓練者未能接觸到的數據進行驗證，以決定某個模型更新是否有效。這種基於投票的共識機制需要系統滿足以下條件才能正常運行：
- 超過2/3的驗證者是非拜占庭(non-Byzantine)節點（即無惡意行為）。
- 驗證者數據集之間的相關性應足夠強，能夠達成有效共識。
- 底層網路至少部分同步。
- 系統生成模型的質量取決於在訓練和驗證過程中使用的有用數據總量。

>[!Note]
> 系統的集中化程度對於選擇適合此架構的梯度下降或其他優化技術有重要影響。

由於本研究專注於分散式學習，因此只考慮分散式的集中化模型和其完全去中心化的對立面。傳統的梯度下降方法無法在此分散式系統中使用，因此應使用其隨機變體來近似真實梯度，並考慮其收斂速度。

類似於大多數聯邦學習系統，作者的系統使用聯邦平均法（Federated Averaging）來進行平行隨機梯度下降（P-SGD）。這種方法基於簡單的假設：不同的節點在不同數據集上進行訓練，並將更新彙總或平均到單一模型，最終將收斂為有效的全局模型。

### Synchronization

分散式學習過程中，一個關鍵在於分散式模型更新需同步進行，以防止參數不同步導致的模型品質下降。如同其他平行或大規模平行系統，分散式學習系統中應用到「同步屏障」(synchronization barrier)的概念。每次屏障同步標誌著一次更新共享回合，在此回合中，節點會將更新彙總至更新後的模型。同步屏障的頻率越高，模型收斂越快，且品質下降越少。然而，這樣的高頻率同步會帶來較高的通訊成本，每個同步屏障對網路和節點頻寬都有所消耗。整體流程可分為兩個階段：計算階段和共享計算結果的通訊階段。

以下介紹幾種常見的節點同步技術。

**批次同步平行（Bulk Synchronous Parallel, BSP）：** 
- BSP為最簡單的方式，通過計算和通訊的交替進行來保持一致性。其主要優勢在於一致性最好（因此收斂最快）
- 瓶頸
    - 已完成的節點必須等待所有工作節點在每個同步屏障處完成計算。
- 作者的BSP變體中，每一回合的訓練有固定期限，在期限截止後，不再接受該回合的進一步訓練更新。

**延遲同步平行（Stale Synchronous Parallel, SSP）：** 
- 類似於BSP，但同步屏障較為寬鬆，允許在預設次數之外額外進行一些疊代次數。一旦這個寬限期結束，所有工作節點將暫停。
- SSP在低至中等的延遲情況下可確保良好的收斂，但過多的延遲會導致收斂速率下降。
- 在作者的設計中，SSP視作BSP的一種變體，允許根據尚未完成訓練的節點比例（稱為「鬆弛比 slack ratio」）決定期限延長的幅度。已完成訓練的節點則可在期限內繼續訓練最多N步，直至延長期限結束。

**無屏障異步平行（Barrierless Asynchronous Parallel, BAP）：** 
- 位於BSP的相對端，幾乎完全消除了同步成本，允許節點間無等待的異步通訊，帶來極低的開銷（因此加速效果更好）
- 缺點: 可能因延遲增加導致收斂緩慢甚至錯誤。
- 在作者的BAP變體中，訓練者可以在每一回合的期限內持續進行訓練，並定期與驗證者共享成果。一旦提交的訓練量達到最低比率，即釋出新模型，此時訓練者需拉取新模型進行訓練。

**近似同步平行（Approximate Synchronous Parallel, ASP）：** 
- 作者的系統不支援ASP
- ASP並非限制延遲，而是著重於限制參數精度，透過忽略不重要的更新來降低同步成本。
- 缺點: 難以判斷哪些參數不重要，且實現上更為複雜。

### Byzantine Fault-Tolerance
分散式系統特別容易受 Byzantine 影響。作者集中探討懶惰（lazy）和惡意的訓練者行為，因為惡意的驗證者行為多半可由區塊鏈的分散式共識加以解決。為此，作者採用了獎懲政策，為每位訓練者  i  分配一個信任分數（或聲譽） $\phi_i$ ，其中：

1. 在此處，「懶惰」包含未對模型做出有效貢獻的節點。
$$
0 \leq \phi_i \leq 1 ,
 \sum_{i=1}^{n} \phi_i = 1 
$$

#### 訓練者的信任分數
- 目的: 用作聯邦學習算法中該訓練者更新的權重。
- 效果
    - 讓驗證者能夠根據訓練者的信任等級來控制其更新對模型的影響。
    - 它在系統內部創造了內在的競爭，因為一個訓練者分數的提升會相對降低其他訓練者的分數。
- 信任分數會根據驗證結果進行調整。
- 當驗證者收到訓練者更新時，會將其梯度添加至最新的模型權重並計算驗證分數，然後根據更新是否改善模型或導致性能下降來調整該訓練者的信任分數。

## ARCHITECTURE
### System Overview

FLoBC 是一個分散式系統，主要由兩個角色驅動：訓練者和驗證者，其中驗證者負有更複雜的職責。在更低層級上，系統由六個主要服務組成，每個服務提供特定功能並與其他服務和層級有嚴格定義的接口。

1. **區塊鏈服務**：構成所有驗證節點之間的主要通訊和數據存儲結構。為確保所有驗證者數據一致性，該子系統使用了一種去中心化共識機制的複雜架構，採用改良的實用拜占庭容錯（Practical Byzantine
Fault Tolerance , pBFT）算法，主要依賴於投票和選舉。
2. **存儲結構服務**：促進並實現本地存儲資料庫的結構化訪問和修改，用於存儲區塊鏈狀態。
3. **機器學習服務**：代表區塊鏈之上的機器學習層，作為一個服務來處理機器學習交易，例如通過與本地存儲結構交互以執行交易操作，如模型創建和模型聚合。
4. **訓練驗證服務**：位於區塊鏈和訓練層之間的混合層，負責驗證傳入的更新並決定是否接受或拒絕梯度交易。
5. **信譽管理服務**：負責系統的計分管理，通過獎懲機制確保僅誠實的訓練者能留在網路中，拜占庭行為者則會被移除，以保持創建模型的性能和質量，並確保公平性。
6. **模型訓練服務**：執行模型訓練，包括訓練者端所需的所有中間轉換，例如模型扁平化和重建。此層負責發送和接收扁平化的模型和梯度。
7. **訓練流管理服務**：管理訓練者和驗證者之間的通訊，包括模型和梯度交換，以及訓練迭代的同步。

### Implementation Technology
FLoBC 架設於 Exonum 區塊鏈之上，並採用改良版的實用拜占庭容錯（pBFT）共識算法。選擇此算法的主要原因在於 Exonum 的投票式共識比典型的加密工作量證明（PoW）共識輕量得多。當考慮到模型訓練本身已經相當耗費計算資源時，這種輕量共識變得尤為重要。系統中的大部分服務均使用 Rust 程式語言實現，而輕量級客戶端則使用 JavaScript 開發。此外，訓練和驗證過程則透過易於插入的 Python 腳本執行。

### System Views


1) **角色層級視角**：這是從系統主要運作角色及其連結關係來進行最高層次的描述。在 FLoBC 系統中，有兩種類型的角色：訓練者（trainer）和驗證者（validator）。訓練者主要負責使用他們的數據進行梯度計算，是系統的基礎勞動力，而驗證者則承擔更複雜的工作，如進行選舉達成共識並驗證以確保模型的質量。

> 作者架構的一個重要特性是，訓練者並不與所有驗證者完全連接，而驗證者之間則是完全互聯的。需要注意的是，這些角色（如訓練者或驗證者）並非固定分配的，一個節點不能在同一子網中同時擔任訓練者和驗證者的角色，但可以在不同子網中分別擔任不同角色。此外，每個模型子網至少需要一名驗證者才能正常運作。

2) **模組與服務層級視角**：
    - 驗證者內部的工作流程
        - 模組視角
            - 顯示了各高層模組之間的數據流。
            - ![image](https://hackmd.io/_uploads/r1x78sXWke.png)
        - 首先，訓練者通過 HTTP 與驗證者通信，這些請求會被轉送至驗證模組，該模組利用驗證數據集來驗證模型更新交易，並最終決定是否將更新納入區塊鏈。驗證者還提供 Wire API，以便查詢區塊鏈狀態（例如最新的模型版本），這對系統診斷和監控有幫助。
    - 服務層級視角
        - 系統中各服務的職責分工
        - 儲存結構服務是驗證者端所有服務的核心，作為進行持久性更改的主要介面。機器學習（ML）服務負責整合訓練者的更新並處理訓練流程（例如同步），而驗證服務則負責評估訓練者的工作並將結果提供給信譽管理服務，根據結果調整信任分數。兩側的 Wire API 為其他查詢接口，提供各種用途（例如查詢最新發布的模型版本）以及主要用於發送交易的區塊鏈接口。![image](https://hackmd.io/_uploads/HklEIim-Jx.png)

## EXPERIMENTS
- Benchmark: Decentralized vs. Centralized Performance

### Experiment 1: Trainers-to-Validators Ratio

#### 目的
- 平衡訓練者和驗證者數量之間的權衡。
    - 希望確定最佳的訓練者與驗證者的分配，以便充分利用計算資源並實現最佳的模型性能，同時仍依賴多個驗證者來生成更可信的模型。
    - 顯示達到最高準確度的收斂速度，並指出達到最高準確度的迭代次數


#### 實驗設計背景

>[!Important]
> 增加擁有良好數據的訓練者數量應該會提高模型性能。
 
從貪婪的角度會希望系統中的所有節點都作為訓練者，然而這存在一個主要的缺點: 依賴單一的驗證者意味著該驗證者是一個可信的實體，而依賴多個驗證者則通過共識提高了模型的可信度。

為了避免浪費原本可以用於訓練的計算資源，並增加不必要的通訊成本，作者避免擁有過多的驗證者。每個訓練者與一個驗證者共享其更新，該驗證者驗證這些更新，然後再將其分享給其他驗證者進行進一步的驗證，最終達成共識。隨著增加驗證者的數量，與投票共識相關的通訊成本也會增加。因此，為了達到最高的效率和最佳的性能，確定最佳的訓練者與驗證者比例以實現最佳的模型質量而不妥協去中心化是非常重要的。

#### 設置
最多 N 個參與節點。在每次運行中，將 N 拆分為不同數量的訓練者和驗證者。

在這個實驗中，系統中的代理數量 N 設置為 10。例如，當 N = 10 時，一次系統運行有 9 個訓練者和 1 個驗證者，而另一個運行則有 8 個訓練者和 2 個驗證者，等等。在每次運行中，我們改變訓練者和驗證者的數量，以嘗試所有加起來為 10 個代理的組合。每次運行持續 30 次迭代。我們記錄每次系統運行的迭代次數及其準確率。由於在此實驗中所有訓練者必須提交他們的工作以進行驗證，因此新模型版本的創建需要等待所有訓練者將其更新共享給驗證者，這一點非常重要，因為我們正在研究驗證者和訓練者的分配對模型性能的影響。

#### 結果
如圖 4 所示，這個模型的最佳配置是有 3 個驗證者和 7 個訓練者。使用這種配置，最大準確率 0.9874 在第 26 次迭代中達到。

>[!Important]
>單個訓練者配置的準確率最低，因為為訓練投入的計算資源和數據少得多。

![image](https://hackmd.io/_uploads/r1VZdi7W1g.png)

### Experiment 2: Reward-Penalty Policy

#### 實驗目的
這個實驗的目的是評估為每個訓練者計算分數的有用性，其中訓練者的分數影響該訓練者的更新對整體模型的影響程度。

#### 實驗設置
這個實驗是使用代表 6 個訓練者（編號從 0 到 5）和 3 個驗證者的進程進行的。訓練運行了 30 次迭代。使用了 BSP 同步方案，選擇了足夠長的週期以允許所有訓練者在每個同步週期提交更新。每個訓練者的更新都使用大約正態噪聲進行偏移。噪聲的均值為 0，標準差與訓練者的索引成線性比例，比例常數 k = 0.0545。因此，訓練者 0 的更新沒有噪聲，而訓練者 5 的更新則受到顯著的噪聲影響。為了這個實驗，更新的最低接受閾值被大幅降低，以允許表現不佳的更新影響模型生成並受到計分的影響。對照組禁用了計分；也就是說，所有 6 個訓練者的分數在整個訓練過程中是恆定且相等的。計分組則允許每個訓練者的分數變化，以適應每個訓練者所提供更新的相對質量。

#### 結果與討論
在計分組運行過程中，每個訓練者的分數在每個同步週期後進行更新。表 I 顯示了 30 次訓練迭代中所有訓練者分數的樣本。

![image](https://hackmd.io/_uploads/SJOmGhXWkl.png)

在訓練開始不久後，驗證者意識到大多數訓練者提供的更新質量較低，因此四個訓練者的分數幾乎立即降至零。在剩餘的迭代中，訓練者 0 的分數穩步上升，因為其更新未受附加噪聲的影響，而第二最佳訓練者 1 的更新受到了最小量的噪聲影響。因此，作者得出結論，計分使驗證者能夠準確評估每個訓練者的相對表現。

對於這兩組，在每次訓練回合後，計算當前模型的準確率。兩組的準確率表現如圖 5 所示。訓練者計分改善了模型的性能、穩定性和收斂速度，因為只有高性能訓練者的更新被考慮並有助於改善模型。

![image](https://hackmd.io/_uploads/BygWznQZ1g.png)


### Experiment 3: Synchronization Schemes

在這個實驗中，作者比較了一些常見的並行計算同步方案，並對它們進行了適當的變化。FLoBC 中實現的方案以及在實驗中測試的方案包括大容量同步並行（BSP）、過時同步並行（SSP）和無障礙異步並行（BAP），如前述方法論所描述。這個實驗的目的是從兩個主要指標來比較不同的方案：模型增長相對於訓練回合的數量，以及每個訓練回合的平均時間。前者指示收斂，而後者指示進展速度。

#### 實驗設置
所有運行都使用三個驗證者和六個訓練者的系統，並採用統一的信任政策；即所有訓練者的信任分數相同。對於 BSP 和 SSP，使用相同的基礎同步屏障週期。不同的訓練者以不同的速度運行，以模擬現實世界中的計算差異，同步屏障週期設定為長於大多數（但不是全部）訓練者完成一個訓練任務所需的時間。

為了測量模型增長指標，系統將運行固定的 N=30 次訓練回合，並比較每個方案在達到最高模型準確率方面所需的時間，同時考慮三個最高的準確率。至於平均訓練迭代延遲，系統將運行固定的 20 分鐘。對於 BAP，我們使用兩種配置：一種是懈怠比率閾值為 0%（完全放鬆），另一種為 40%。也就是說，在第一種情況下，當所有訓練者在當前訓練回合中提交了至少一個更新時，就會觸發同步屏障，而第二種情況則僅依賴於 60% 的訓練者完成這一操作。

#### 結果
如圖 6 所示，結果似乎符合理論預期，確認了三種不同方案之間的權衡。
![image](https://hackmd.io/_uploads/HkbrXhmbye.png)

BSP 的性能最低，因為它在同步方面是最嚴格的，這導致了訓練能力的低利用率。然而，BSP 相當穩定，因為它在進展速度（在固定時間內）上提供了更可預測的限制。至於兩次 BAP 的運行，BAP 1（佔有率為 100%）預期能獲得最佳性能，因為它對訓練者工作的利用最為充分，超過了其 BAP 0.6 對應版本，這確認了我們 BAP 變體中降低佔有率的預期效果。另一方面，SSP 似乎在 BAP 和 BSP 之間達成了一種平衡。這是預期中的結果，因為 SSP 被視為嚴格同步（BSP）和完全放鬆（BAP）之間的折衷方案。

查看圖 7，完全放鬆的 BAP 執行的迭代（即訓練回合）數量遠遠最少，這是因為它的同步屏障比較放鬆。自然地，部分放鬆的 BAP 成功執行了顯著更多的回合，最終使其在所有方案中獲得了最高準確率。另一方面，BSP 的進展最為遠，因為它的速度更快，成功接近了最高準確率。SSP 執行的訓練迭代次數少於 BSP，因為它對同步屏障施加了更多的放鬆。然而，由於其對訓練能力的更好利用，SSP 在準確性方面仍然超過了 BSP。![image](https://hackmd.io/_uploads/Sy8wXhmZyl.png)


總的來說，這些方案中的每一個都適合特定的系統配置（和目的），取決於多種因素，但主要的決定因素是訓練者的速度和質量。


## 結論
總結來說，FLoBC 是一個概念驗證，旨在顯示其組件可以結合成為一個具有所需一般性、去中心化、效率、隱私和拜占庭容錯能力的連貫系統。通過我們的基準實驗，我們的去中心化系統已證明可以與純中央訓練相媲美，並且在性能上略微超過了中央基準，增幅為 0.5%。使用由 FLoBC 產生的簡單系統，我們首先展示了在訓練者與驗證者的劃分中存在一個準最佳平衡，實證確定 7:3 的訓練者與驗證者比例效果最佳。其次，我們展示了即使是簡單的獎勵-懲罰政策也可以對生成模型的質量產生顯著的正面影響。最後，我們比較了三種主要的同步方案（BSP、SSP 和 BAP），突顯並對比了它們之間的不同權衡。


## 未來
儘管模型的 SGD 兼容性是系統中嵌入的假設，但 FLoBC 仍然可以合理地擴展以處理其他計算模型。此外，可以通過根據大多數訓練者在一輪中提交更新所需的預期時間，允許同步週期有更大的適應性，來進一步改進同步方案。此外，為了確保更高水平的數據隱私，可以在交易通信中加入差分隱私層，以限制梯度被反演以提取訓練者數據的程度。
