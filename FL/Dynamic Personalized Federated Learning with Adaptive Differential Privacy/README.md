# Dynamic Personalized Federated Learning with Adaptive Differential Privacy


- 原文 https://papers.neurips.cc/paper_files/paper/2023/file/e4724af0e2a0d52ce5a0a4e084b87f59-Paper-Conference.pdf
- 時間 2023
- Github: https://github.com/xiyuanyang45/DynamicPFL



# Introduction



# Related Works
## Personalized Federated Learning 個人化聯邦學習  
- 意思: 模型的一部分被個人化以學習客戶特定的知識，而其餘部分則捕捉所有客戶之間的全局模式 
- 演算法
    - FedAvg
        - 本地資料的非獨立同分布（non-IID）特性促進了個人化聯邦學習（PFL）的誕生。在 PFL 中，
- PFL
    - LG-FedAvg, FULR 
        - 利用固定的本地參數來提取本地資料的表徵，從而實現個人化
        - 其靜態分區方法缺乏適應不同資料特性的靈活性。
    - FedPer, FedBABU, PPSGD
        - 在本地保留特定層來實現個人化，這樣可以更好地迎合本地資料特性
        - 在模型個人化方面存在潛在的不靈活性
    - FedSim 和 FedAlt
        - 分析了本地和全球參數的不同更新範式，幫助理解本地資料特異性和全球資料共性之間的權衡

> [!Caution]
> 以上演算法的問題: 在個人化方法的不靈活性以及在差分隱私（DP）機制下直接使用含有噪聲的全球參數進行訓練導致的潛在性能下降方面存在局限性

作者提出了一種基於逐層費雪信息值的創新個人化聯邦學習方法，實現了動態個人化，超越了固定參數分區的不靈活性，同時減少了噪聲的影響。




## Differential Privacy
- 方法
    - 將個別資料變動對計算輸出的影響降到最低
    - (ε, δ)-差分隱私來量化
        - 較小的 ε 和 δ 提供更強的隱私保護，但會在學習演算法中引入更多的噪聲，這可能會降低性能。
- 在聯邦學習中，使用者級 DP 通過 DP 機制內在的兩步過程實現：
    - 在傳送到伺服器之前，加入噪聲並裁剪本地更新。
        - 噪聲的規模根據函數的敏感度來校準。
        - 裁剪則限制了本地更新的影響，進一步增強了隱私。
    - 問題: 性能下降和收斂速度變慢等挑戰。
    - 解決: 
        - LUS、BLUR
            - 稀疏化和統一正則化技術來減輕噪聲影響並增強模型的收斂性
        - DP-FedSAM
            - 使用 SAM 優化器來增強參數的抗噪聲能力，旨在識別更穩定的收斂點
        - PPSGD。
            - 利用個人化來提高性能，同時保持隱私。
        
>[!Caution]
>上述方法未能充分解決噪聲和裁剪在差分隱私中的問題。
>1. 所有參數在訓練過程中都會受到噪聲的影響，導致模型性能下降
>2. 對裁剪約束的統一處理未能適應本地更新的動態性，進一步阻礙了模型的學習和收斂

作者提出了一種新方法，利用逐層費雪信息來動態個人化並保護具有高信息含量的參數免受噪聲影響，並引入了一種自適應正則化策略，對個人化和共享參數施加差分約束，增強模型的學習能力和對裁剪的韌性。



## Fisher Information Matrix
Fisher 資訊矩陣（Fisher Information Matrix, FIM）
- 應用
    - 統計估計理論中的一個核心概念，表徵未知參數對隨機分佈所包含的信息量。
    - 在深度學習領域中，FIM 被用來研究損失函數的曲率、指導優化過程，並評估參數的信息量。
        - 例如，Kronecker-Factored Approximate Curvature (K-FAC) 方法使用 Kronecker 乘積來近似 FIM，以更高效地進行自然梯度計算。層級相關性傳播方法利用 FIM 的對角線來量化特徵的重要性，從而增強模型的可解釋性。彈性權重鞏固算法（Elastic Weight Consolidation）則使用 FIM 來保護在學習新任務時的重要參數，從而減輕災難性遺忘。這些方法均採用 FIM 的對角線作為近似，降低了計算複雜度並促進了更高效的學習過程。

作者的新方法將層級 Fisher 資訊融入個性化聯邦學習，利用其動態確定個性化參數，從而保護重要的參數免受差分隱私引起的噪聲干擾。這一策略有效地減少了噪聲的影響，提升了個性化模型的性能。此外也引入了一種自適應方法，根據 Fisher 資訊值的大小動態調整 L2 範數約束。該方法增強了對剪裁操作的魯棒性，提供了一種在具備差分隱私的個性化聯邦學習中平衡模型學習與隱私保護的更靈活解決方案。


# Methodology
##  Preliminary
### 個性化聯邦學習（Personalized Federated Learning, PFL）
- 目標: 解決現實世界中數據分佈的非獨立同分佈（non-IID）特性。
- 方法
    - 在 PFL 中，客戶端模型的參數向量被分解為本地部分 \(u\)，適應個別數據特性，以及全局部分 \(v\)，由所有客戶端共享。
- 說明
    - 假設有 \(M\) 個客戶端，每個客戶端擁有一個唯一的私有數據集，記為 $D_m = \{(x_i, y_i)\}_{i=1}^{N_m}$，其中 $N_m$ 代表第 \(m\) 個客戶端的本地數據集大小。
    - 客戶端 \(i\) 的模型參數向量 $w \in \mathbb{R}^{d_w}$ 被分解為本地部分和全局部分，形成 $w_i = (v, u_i)$。
- PFL 的優化問題定義如下：
$$
\min_{v, u_{1:m}} (f(v, u_{1:m}) := \frac{1}{m} \sum_{i=1}^{M} f_i(v, u_i) ) \tag{1}
$$
    - 其中，$u_{1:m}$ 代表 $(u_1, \ldots, u_m) \in (\mathbb{R}^{d_u})^M$
    - $f_i(v, u_i) := \mathbb{E}_{D_i \sim P_i}[f_i(v, u_i, D_i)]$ 是第 \(i\) 個客戶端的期望風險。
- PFL 使用兩個步驟的迭代來解決這個問題：
    - **本地更新**：在本地更新階段，每個客戶端 \(i\) 採用最新的全局參數 $v^t$，同時保留其上一輪的本地參數 $u_i^t$ 初始化模型 $w_i^t = (v^t, u_i^t)$，然後執行 $E_{local}$ 次本地更新，得到新的參數 $w_i^{t+1} = (v_i^{t+1}, u_i^{t+1})$。隨後，他們通過計算 $v_i^{t+1}$ 與 $v_i^t$ 之間的差異來計算本地更新 $\Delta v_i^{t+1}$。
    - **協作更新**：在協作更新階段，所有客戶端將其本地更新 $\Delta v_i^{t+1}$ 發送至服務器。服務器對這些參數取平均值，更新全局參數 $v^{t+1} = v^t + \frac{1}{M} \sum_{i=1}^{M} \Delta v_i^{t+1}$，並將其分發給所有客戶端，為下一輪本地更新做準備。在我們的方法中，由於共享的參數部分是動態選擇的且無法事先確定，因此所有客戶端會將其整個更新 $\Delta w_i^{t+1}$ 發送至服務器。

### 用戶層級差分隱私
- 在PFL的背景下，為防止潛在的隱私洩露，採用差分隱私（Differential Privacy, DP）
- DP
    - 提供理論保證的正式隱私框架，可防止數據集中私有數據的識別。
    - 元素說明:
        - $\epsilon$ : 隱私預算，量化隱私保護級別
        - $\delta$ : 隱私保證被違反的概率。
        - M: 隨機算法，滿足$(\epsilon, \delta)-DP$
            - 如果對於僅相差一條記錄的任意相鄰數據集 \(D\) 和 \(D'\)，以及 \(M\) 範圍內的任意輸出子集 \(S\)，都有：
$$
\Pr[M(D) \in S] \leq e^\epsilon \Pr[M(D') \in S] + \delta \tag{2}
$$
            - 當輸入數據集中單條記錄被修改時，\(M\) 的輸出概率分佈僅會輕微變化，從而保護個人隱私。
- 用戶層級 DP 的影響: 使得無論某個客戶端是否參與學習，全局模型更新都難以區分
- 實現方法
    - **剪裁**控制單個客戶端對全局更新的最大貢獻，即使存在異常值，仍能保證 DP 屬性，從而維護個人在學習過程中的隱私。
    - **加噪**則是為了滿足 DP 的隨機性要求，通常從零均值的高斯分佈中抽樣噪聲，難以推斷任何單個客戶端的具體信息。
$$
\Delta w_C = \Delta w \cdot \max\left(1, \frac{\|\Delta w\|_2}{C}\right)\tag{3-1}
$$
$$
\quad \Delta w_{C,N} = \Delta w_C + N(0, C^2\sigma^2/|M_t|) \tag{3-2}
$$
    - $\Delta w$: 是客戶端的原始本地更新
    - $|M_t|$ 是第 \(t\) 輪的可用客戶端數量
    - $C$: 控制單個客戶端對全局更新最大貢獻的剪裁閾值
    - $\Delta w_C$ 是剪裁後的本地更新
    - $\Delta w_{C,N}$ 是剪裁並加噪後的最終本地更新
    - N: 為確保 DP 而添加的高斯噪聲
    - $\sigma$ 是由隱私會計和組成機制根據 $\epsilon$ 和 $\delta$ 計算的噪聲乘數。

## Dynamic Personalization Strategy
### Fisher 信息的動機
Fisher 能夠量化參數所提供的信息量，信息量更大的參數能更有效地表達知識。
>[!Note] 
>在相同的噪聲干擾下，如果影響到的是這些信息量較大的參數，將會導致模型性能的更嚴重退化。
    
我們希望避免噪聲影響信息量大的參數。

- 問題: 當前的 DP-PFL 方法在於個性化 -> 固定，不靈活
    - 在傳統個性化工作裡，在本地更新階段的開始，客戶端直接將全局參數放置在固定位置進行訓練，如此不靈活的個性化方法未考慮噪聲對不同參數的影響，使模型更易受噪聲干擾。
- 解決: 利用 Fisher 信息的動態個性化，以增強模型對個別客戶端數據分佈的適應性
- 方法
    - 考慮了神經網絡的分層結構，並引入了逐層的 Fisher 信息。我們測量每個參數的逐層 Fisher 信息，以反映模型對參數變化的敏感度，從而通過避免噪聲干擾來優化模型。
    
### 動態 Fisher 個性化的構建
在每個全局週期 t 的本地更新階段開始時，客戶端 i 擁有其私有數據集 $D_i$ 和從上個週期保留下來的參數 $w^{t-1}_i = (v^{t-1}, u^{t-1}_i)$。我們可以得到經驗 Fisher 值向量 $F_i \in \mathbb{R}^{d_w}$，其中 $d_w$ 代表參數數量，這是一個參數的真實 Fisher 值對角線的良好近似，公式如下：

$$
F(w_{ij}) = \left( \frac{\partial \log L(w_i, D_i)}{\partial w_{ij}} \right)^2 \tag{4}
$$

其中，$\log L(w_i, D_i)$ 代表 $w_i$ 的 log-likelihood 函數。然後，通過對每層參數 \(j\) 進行層級標準化，得到每層的 Fisher 值：
$$
\hat{F}_{k,j} = \frac{F_{k,j} - \min \{F_{k,j}\}}{\max \{F_{k,j}\} - \min \{F_{k,j}\}} \tag{5}
$$

### 動態參數選擇與更新

根據逐層 Fisher 值，我們為每個參數生成兩個二元掩碼(mask) $M_1$ 和 $M_2$ 來執行動態參數選擇。在每個掩碼中，如果 Fisher 值大於等於閾值 $\tau$，則對應的參數條目設置為 1，否則設置為 0。公式如下：

$$
M_1[j] = \begin{cases} 
1, & \text{if } \hat{F}_{ij} \geq \tau \\
0, & \text{otherwise}
\end{cases}, and \quad
M_2[j] = \begin{cases} 
0, & \text{if } \hat{F}_{ij} \geq \tau \\
1, & \text{otherwise}
\end{cases} \tag{6}
$$

然後對這些掩碼與參數進行 Hadamard 乘積（逐元素乘法），以選擇適合個性化的參數。具體來說，具有較大 Fisher 值的參數（對應於 $M_1$ 中的 1）將保留自上一個週期，而其餘參數（對應於 $M_2$ 中的 1）將由全局參數替代：

$$
w^t_i = M_1 \odot w^{t-1}_i + M_2 \odot w^{t-1}
$$

- $\odot$ 表示 Hadamard 乘積
- $w^{t-1}$ 是從 server 下載的全局參數。

### 與類似方法的比較
- PPSGD 
    - 使用固定層個性化方法和附加模型實現差分隱私（DP）下的個性化
- LUS 
    - 在本地訓練後的稀疏化過程中使用掩碼。
- 作者方法
    - 在訓練過程之前應用掩碼進行動態個性化，旨在提供更靈活、適應性更強的個性化策略，能夠更好地應對數據分佈、噪聲水平和其他影響模型性能的動態因素變化。
    - 保留對模型性能有重大貢獻的最有信息的參數，同時允許較不重要的參數用全局參數更新，從而減少噪聲的影響。
    - 這種動態的訓練前個性化策略相比靜態的訓練後個性化方法具有獨特的優勢。
    
## Adaptive Constraint



# 結論
- 原先挑戰
    - 差分隱私所導致的個人化僵化和收斂困難的挑戰
- 論文方法解決
    - FedDPA，包含兩個創新組件：動態費雪個人化（Dynamic Fisher Personalization, DFP）和自適應約束（Adaptive Constraint, AC）。
    - 這些組件利用逐層的費雪信息來動態個人化並自適應地約束參數，有效解決了上述問題。

